{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64cf740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT max값들 중 최소값: 1389.000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "# CT 파일이 있는 폴더 경로\n",
    "data_root = \"/SSD5_8TB/Daniel/09_registformer_hugging_face/mem_eff_registformer/data/synthrad2023_mr-ct_pelvis\"\n",
    "\n",
    "# 환자 리스트\n",
    "patients = sorted(os.listdir(data_root))\n",
    "patients = [p for p in patients if os.path.isdir(os.path.join(data_root, p))]\n",
    "\n",
    "# CT max 값들 저장용 리스트\n",
    "ct_max_values = []\n",
    "\n",
    "for patient in patients:\n",
    "    ct_path = os.path.join(data_root, patient, \"ct.nii.gz\")\n",
    "    if not os.path.exists(ct_path):\n",
    "        continue\n",
    "\n",
    "    ct_data = nib.load(ct_path).get_fdata()\n",
    "    ct_max = ct_data.max()\n",
    "    ct_max_values.append(ct_max)\n",
    "\n",
    "# 모든 CT max값들 중 최소값 출력\n",
    "if ct_max_values:\n",
    "    min_of_max = min(ct_max_values)\n",
    "    print(f\"CT max값들 중 최소값: {min_of_max:.3f}\")\n",
    "else:\n",
    "    print(\"CT 파일이 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ee28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54562b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output dir: /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값\n",
      "[LOAD] ProposedSynth_Epoch87_fullimage: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registformer_MrCtPelvis_ver2_ProposedSynthesis_NoRandTranslation_VoxelmorphProposedSynth_Test_Epoch87_fullimage/preds_a_1PA004.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_ProposedSynth_Epoch87_fullimage.csv\n",
      "[LOAD] MUNIT_Lcx_rotate: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/I2I_translation/MUNIT+Lcx(예전)_rotate/preds_b_1PA004.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_MUNIT_Lcx_rotate.csv\n",
      "[LOAD] VoxelMorph2D_warped: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registration/voxelmorph2D/warped_img_1PA004.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_VoxelMorph2D_warped.csv\n",
      "[SAVE] Summary -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/inter_slice_consistency_summary.csv\n",
      "[PLOT] SSIM_global -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_SSIM_global.png\n",
      "[PLOT] PSNR -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_PSNR.png\n",
      "[PLOT] MI -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_MI.png\n",
      "[PLOT] NCC -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_NCC.png\n",
      "[PLOT] TV -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_TV.png\n",
      "\n",
      "=== Summary (lower is better for TV metrics; higher is better for PSNR/SSIM/MI/NCC) ===\n",
      "                           Name  NumSlices     TV_z    TV_xy  nTV_z_by_IQR  nTV_z_by_TVxy  Edge_TV_z  Lap_TV_z  HF_LaplacianAbs_mean  HF_LaplacianVar_mean  HF_EdgeMag_mean  PSNR_mean  SSIM_global_mean  MI_mean  NCC_mean  TV_interval_mean\n",
      "ProposedSynth_Epoch87_fullimage        119 0.016117 0.014026      0.139450       1.149070   0.067126  0.027929              0.028992              0.002808         0.147710  30.373850          0.921217 1.041238  0.915458          0.016446\n",
      "               MUNIT_Lcx_rotate        119 0.015819 0.010870      0.146988       1.455379   0.061873  0.021849              0.020747              0.001530         0.119119  29.691340          0.917663 1.102263  0.912451          0.016175\n",
      "            VoxelMorph2D_warped        119 0.011405 0.011887      0.095209       0.959465   0.049749  0.016637              0.021127              0.001772         0.136170  32.661242          0.959032 1.358329  0.956585          0.011582\n"
     ]
    }
   ],
   "source": [
    "# Unified inter-slice consistency script (local-only)\n",
    "# - TV/nTV, Edge-TV, Lap-TV, in-plane sharpness, Otsu mask, spacing_z\n",
    "# - Summary & per-slice CSVs + plots\n",
    "# - Keeps original absolute paths and filenames\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- I/O backends ----\n",
    "nib = None\n",
    "sitk = None\n",
    "try:\n",
    "    import nibabel as nib\n",
    "except Exception:\n",
    "    nib = None\n",
    "if nib is None:\n",
    "    try:\n",
    "        import SimpleITK as sitk\n",
    "    except Exception:\n",
    "        sitk = None\n",
    "\n",
    "# ---- Optional filters ----\n",
    "has_scipy = False\n",
    "try:\n",
    "    from scipy.ndimage import sobel, laplace\n",
    "    has_scipy = True\n",
    "except Exception:\n",
    "    has_scipy = False\n",
    "\n",
    "# ---- Paths (KEEP) ----\n",
    "paths = {\n",
    "    \"ProposedSynth_Epoch87_fullimage\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registformer_MrCtPelvis_ver2_ProposedSynthesis_NoRandTranslation_VoxelmorphProposedSynth_Test_Epoch87_fullimage/preds_a_1PA004.nii.gz\",\n",
    "    \"MUNIT_Lcx_rotate\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/I2I_translation/MUNIT+Lcx(예전)_rotate/preds_b_1PA004.nii.gz\",\n",
    "    \"VoxelMorph2D_warped\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registration/voxelmorph2D/warped_img_1PA004.nii.gz\",\n",
    "}\n",
    "# Base output dir (KEEP)\n",
    "base_out_dir = r\"/SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값\"\n",
    "plots_dir = os.path.join(base_out_dir, \"jbhi_metrics\")\n",
    "os.makedirs(base_out_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def load_nifti(path):\n",
    "    if nib is not None:\n",
    "        img = nib.load(path)\n",
    "        return img.get_fdata(dtype=np.float32)  # (X,Y,Z)\n",
    "    elif sitk is not None:\n",
    "        img = sitk.ReadImage(path)\n",
    "        arr = sitk.GetArrayFromImage(img)  # (Z,Y,X)\n",
    "        return np.transpose(arr, (2,1,0)).astype(np.float32)\n",
    "    else:\n",
    "        raise RuntimeError(\"nibabel 또는 SimpleITK가 필요합니다.\")\n",
    "\n",
    "def robust_minmax_norm(vol, p1=1.0, p99=99.0, eps=1e-6):\n",
    "    lo = np.percentile(vol, p1)\n",
    "    hi = np.percentile(vol, p99)\n",
    "    vol = np.clip(vol, lo, hi)\n",
    "    return (vol - lo) / max(hi - lo, eps)\n",
    "\n",
    "def otsu_threshold(arr, bins=256):\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return 0.0\n",
    "    hist, bin_edges = np.histogram(arr, bins=bins, range=(arr.min(), arr.max()))\n",
    "    w1 = np.cumsum(hist)\n",
    "    w2 = np.cumsum(hist[::-1])[::-1]\n",
    "    mu1 = np.cumsum(hist * (bin_edges[:-1] + bin_edges[1:]) / 2) / np.maximum(w1, 1e-12)\n",
    "    mu2 = (np.cumsum((hist * (bin_edges[:-1] + bin_edges[1:]) / 2)[::-1]) / np.maximum(w2[::-1], 1e-12))[::-1]\n",
    "    var12 = w1[:-1] * w2[1:] * (mu1[:-1] - mu2[1:])**2\n",
    "    idx = np.argmax(var12)\n",
    "    return float((bin_edges[idx] + bin_edges[idx+1]) / 2)\n",
    "\n",
    "# ---- Metrics ----\n",
    "def gradient_magnitude(s2d):\n",
    "    if has_scipy:\n",
    "        gx = sobel(s2d, axis=0, mode='reflect')\n",
    "        gy = sobel(s2d, axis=1, mode='reflect')\n",
    "    else:\n",
    "        gx = np.zeros_like(s2d); gy = np.zeros_like(s2d)\n",
    "        gx[1:-1,:] = (s2d[2:,:] - s2d[:-2,:]) / 2.0\n",
    "        gy[:,1:-1] = (s2d[:,2:] - s2d[:,:-2]) / 2.0\n",
    "    return np.sqrt(gx**2 + gy**2)\n",
    "\n",
    "def laplacian2d(s2d):\n",
    "    if has_scipy:\n",
    "        return laplace(s2d)\n",
    "    center = 4*s2d\n",
    "    n = np.zeros_like(s2d)\n",
    "    n[1: ,:] += s2d[:-1,:]; n[:-1,:] += s2d[1: ,:]\n",
    "    n[: ,1:] += s2d[:, :-1]; n[: ,:-1] += s2d[:, 1: ]\n",
    "    return n - center\n",
    "\n",
    "def mse(a,b): return float(np.mean((a-b)**2))\n",
    "def psnr_from_mse(m, data_range=1.0):\n",
    "    if m <= 1e-12: return float('inf')\n",
    "    return 20.0*math.log10(data_range) - 10.0*math.log10(m)\n",
    "\n",
    "def global_ssim(a,b,data_range=1.0,K1=0.01,K2=0.03):\n",
    "    a = a.astype(np.float64); b = b.astype(np.float64)\n",
    "    C1 = (K1*data_range)**2; C2 = (K2*data_range)**2\n",
    "    mu_a = float(np.mean(a)); mu_b = float(np.mean(b))\n",
    "    var_a = float(np.var(a)); var_b = float(np.var(b))\n",
    "    cov = float(np.mean((a - mu_a) * (b - mu_b)))\n",
    "    num = (2*mu_a*mu_b + C1) * (2*cov + C2)\n",
    "    den = (mu_a**2 + mu_b**2 + C1) * (var_a + var_b + C2)\n",
    "    if den == 0: return 1.0 if num == 0 else 0.0\n",
    "    return float(num/den)\n",
    "\n",
    "def mutual_information(a,b,bins=64):\n",
    "    a = np.clip(a.ravel(),0,1); b = np.clip(b.ravel(),0,1)\n",
    "    h, _, _ = np.histogram2d(a,b,bins=bins,range=[[0,1],[0,1]])\n",
    "    p = h/np.sum(h) + 1e-12\n",
    "    px = np.sum(p,axis=1,keepdims=True); py = np.sum(p,axis=0,keepdims=True)\n",
    "    return float(np.sum(p*(np.log(p) - np.log(px) - np.log(py))))\n",
    "\n",
    "def ncc(a,b):\n",
    "    a = a.astype(np.float64); b = b.astype(np.float64)\n",
    "    am, bm = np.mean(a), np.mean(b)\n",
    "    asd, bsd = np.std(a), np.std(b)\n",
    "    if asd < 1e-8 or bsd < 1e-8: return 0.0\n",
    "    return float(np.mean((a-am)*(b-bm)) / (asd*bsd))\n",
    "\n",
    "def tv_z(vol, mask=None):\n",
    "    diffs = np.abs(np.diff(vol, axis=2))\n",
    "    if mask is not None:\n",
    "        diffs = diffs * mask[:,:,1:]\n",
    "        denom = np.sum(mask[:,:,1:])\n",
    "        return float(np.sum(diffs) / max(denom, 1e-12))\n",
    "    return float(np.mean(diffs))\n",
    "\n",
    "def tv_z_per_interval(vol, mask=None):\n",
    "    diffs = np.abs(np.diff(vol, axis=2))\n",
    "    if mask is not None:\n",
    "        diffs = diffs * mask[:,:,1:]\n",
    "        denom = np.sum(mask[:,:,1:], axis=(0,1))\n",
    "        denom = np.maximum(denom, 1e-12)\n",
    "        return np.sum(diffs, axis=(0,1)) / denom\n",
    "    return np.mean(diffs, axis=(0,1))\n",
    "\n",
    "def tv_xy(vol, mask=None):\n",
    "    dx = np.abs(np.diff(vol, axis=0))\n",
    "    dy = np.abs(np.diff(vol, axis=1))\n",
    "    if mask is not None:\n",
    "        mx = mask[1:,:,:]; my = mask[:,1:,:]\n",
    "        dx = dx * mx; dy = dy * my\n",
    "        denom = np.sum(mx) + np.sum(my)\n",
    "        return float((np.sum(dx)+np.sum(dy)) / max(denom, 1e-12))\n",
    "    return float(np.mean(dx) + np.mean(dy))\n",
    "\n",
    "def edge_tv_z(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    gm = np.zeros_like(vol, dtype=np.float32)\n",
    "    for z in range(Z):\n",
    "        gm[:,:,z] = gradient_magnitude(vol[:,:,z])\n",
    "    return tv_z(gm, mask)\n",
    "\n",
    "def lap_tv_z(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    lv = np.zeros_like(vol, dtype=np.float32)\n",
    "    for z in range(Z):\n",
    "        lv[:,:,z] = laplacian2d(vol[:,:,z])\n",
    "    return tv_z(lv, mask)\n",
    "\n",
    "def inplane_sharpness_metrics(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    lap_abs, lap_var, edge_m = [], [], []\n",
    "    for z in range(Z):\n",
    "        sl = vol[:,:,z]\n",
    "        if mask is not None:\n",
    "            m = mask[:,:,z] > 0\n",
    "            if not np.any(m):\n",
    "                lap_abs.append(0.0); lap_var.append(0.0); edge_m.append(0.0); continue\n",
    "            lap = laplacian2d(sl)[m]; gm = gradient_magnitude(sl)[m]\n",
    "        else:\n",
    "            lap = laplacian2d(sl).ravel(); gm = gradient_magnitude(sl).ravel()\n",
    "        lap_abs.append(float(np.mean(np.abs(lap))))\n",
    "        lap_var.append(float(np.var(lap)))\n",
    "        edge_m.append(float(np.mean(gm)))\n",
    "    return {\n",
    "        \"LaplacianAbs_mean\": float(np.mean(lap_abs)),\n",
    "        \"LaplacianVar_mean\": float(np.mean(lap_var)),\n",
    "        \"EdgeMag_mean\": float(np.mean(edge_m)),\n",
    "    }\n",
    "\n",
    "def per_slice_pair_metrics(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    d = {\"MSE\":[], \"PSNR\":[], \"SSIM_global\":[], \"MI\":[], \"NCC\":[], \"TV\":[]}\n",
    "    intervals = tv_z_per_interval(vol, mask)\n",
    "    for z in range(Z-1):\n",
    "        a = vol[:,:,z]; b = vol[:,:,z+1]\n",
    "        if mask is not None:\n",
    "            m = (mask[:,:,z] * mask[:,:,z+1]) > 0\n",
    "            a_sel = a[m] if np.any(m) else a\n",
    "            b_sel = b[m] if np.any(m) else b\n",
    "        else:\n",
    "            a_sel = a; b_sel = b\n",
    "        mv = mse(a_sel, b_sel)\n",
    "        d[\"MSE\"].append(mv)\n",
    "        d[\"PSNR\"].append(psnr_from_mse(mv))\n",
    "        d[\"SSIM_global\"].append(global_ssim(a_sel, b_sel))\n",
    "        d[\"MI\"].append(mutual_information(a_sel, b_sel))\n",
    "        d[\"NCC\"].append(ncc(a_sel, b_sel))\n",
    "        d[\"TV\"].append(float(intervals[z]))\n",
    "    for k in d.keys():\n",
    "        d[k] = np.array(d[k], dtype=np.float64)\n",
    "    return d\n",
    "\n",
    "def summarize_all(vol, name, spacing_z=1.0, use_mask=True):\n",
    "    vol_n = robust_minmax_norm(vol)\n",
    "    mask = None\n",
    "    if use_mask:\n",
    "        thr = otsu_threshold(vol_n)\n",
    "        mask = (vol_n > thr).astype(np.float32)\n",
    "\n",
    "    tvz = tv_z(vol_n, mask)\n",
    "    tvxy = tv_xy(vol_n, mask)\n",
    "\n",
    "    nz = vol_n[vol_n > 0]\n",
    "    iqr = float(np.percentile(nz,75) - np.percentile(nz,25)) if nz.size > 0 else 1.0\n",
    "    nTV_iqr = tvz / max(iqr, 1e-6)\n",
    "    nTV_xy  = tvz / max(tvxy, 1e-6)\n",
    "\n",
    "    edge_tv = edge_tv_z(vol_n, mask)\n",
    "    lap_tv  = lap_tv_z(vol_n, mask)\n",
    "\n",
    "    curves = per_slice_pair_metrics(vol_n, mask)\n",
    "    sharp  = inplane_sharpness_metrics(vol_n, mask)\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"NumSlices\": int(vol.shape[2]),\n",
    "        \"TV_z\": tvz / max(spacing_z, 1e-6),  # spacing 보정\n",
    "        \"TV_xy\": tvxy,\n",
    "        \"nTV_z_by_IQR\": nTV_iqr,\n",
    "        \"nTV_z_by_TVxy\": nTV_xy,\n",
    "        \"Edge_TV_z\": edge_tv,\n",
    "        \"Lap_TV_z\": lap_tv,\n",
    "        \"HF_LaplacianAbs_mean\": sharp[\"LaplacianAbs_mean\"],\n",
    "        \"HF_LaplacianVar_mean\": sharp[\"LaplacianVar_mean\"],\n",
    "        \"HF_EdgeMag_mean\": sharp[\"EdgeMag_mean\"],\n",
    "        \"PerSlice_Metrics\": curves,\n",
    "    }\n",
    "\n",
    "# ---- Runner ----\n",
    "def run(paths, base_out_dir, plots_dir, spacing_z=1.0, use_mask=True):\n",
    "    os.makedirs(base_out_dir, exist_ok=True)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    summaries = []\n",
    "    per_slice_csvs = {}\n",
    "\n",
    "    print(f\"[INFO] Output dir: {base_out_dir}\")\n",
    "    for name, p in paths.items():\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Missing file: {p}\")\n",
    "            continue\n",
    "        print(f\"[LOAD] {name}: {p}\")\n",
    "        vol = load_nifti(p)\n",
    "        if vol.ndim != 3:\n",
    "            raise ValueError(f\"{name}: expected 3D, got {vol.shape}\")\n",
    "\n",
    "        S = summarize_all(vol, name, spacing_z=spacing_z, use_mask=use_mask)\n",
    "        summaries.append(S)\n",
    "\n",
    "        # per-slice CSV (KEEP location/pattern)\n",
    "        df = pd.DataFrame({k: v for k, v in S[\"PerSlice_Metrics\"].items()})\n",
    "        per_csv = os.path.join(base_out_dir, f\"per_slice_{name}.csv\")\n",
    "        df.to_csv(per_csv, index=False)\n",
    "        per_slice_csvs[name] = per_csv\n",
    "        print(f\"[SAVE] Per-slice metrics -> {per_csv}\")\n",
    "\n",
    "    if len(summaries) == 0:\n",
    "        raise RuntimeError(\"No volumes processed. Check file paths.\")\n",
    "\n",
    "    # Summary CSV (KEEP filename)\n",
    "    rows = []\n",
    "    for S in summaries:\n",
    "        rows.append({\n",
    "            \"Name\": S[\"Name\"],\n",
    "            \"NumSlices\": S[\"NumSlices\"],\n",
    "            \"TV_z\": S[\"TV_z\"],\n",
    "            \"TV_xy\": S[\"TV_xy\"],\n",
    "            \"nTV_z_by_IQR\": S[\"nTV_z_by_IQR\"],\n",
    "            \"nTV_z_by_TVxy\": S[\"nTV_z_by_TVxy\"],\n",
    "            \"Edge_TV_z\": S[\"Edge_TV_z\"],\n",
    "            \"Lap_TV_z\": S[\"Lap_TV_z\"],\n",
    "            \"HF_LaplacianAbs_mean\": S[\"HF_LaplacianAbs_mean\"],\n",
    "            \"HF_LaplacianVar_mean\": S[\"HF_LaplacianVar_mean\"],\n",
    "            \"HF_EdgeMag_mean\": S[\"HF_EdgeMag_mean\"],\n",
    "            \"PSNR_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"PSNR\"])),\n",
    "            \"SSIM_global_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"SSIM_global\"])),\n",
    "            \"MI_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"MI\"])),\n",
    "            \"NCC_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"NCC\"])),\n",
    "            \"TV_interval_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"TV\"])),\n",
    "        })\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    summary_path = os.path.join(base_out_dir, \"inter_slice_consistency_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"[SAVE] Summary -> {summary_path}\")\n",
    "\n",
    "    # Plots (save into jbhi_metrics subfolder)\n",
    "    def plot_metric(metric_key, ylabel):\n",
    "        plt.figure()\n",
    "        for S in summaries:\n",
    "            y = S[\"PerSlice_Metrics\"][metric_key]\n",
    "            x = np.arange(1, len(y)+1)\n",
    "            plt.plot(x, y, label=S[\"Name\"])\n",
    "        plt.xlabel(\"Adjacent slice index (z → z+1)\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        out = os.path.join(plots_dir, f\"plot_{metric_key}.png\")\n",
    "        plt.savefig(out, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"[PLOT] {metric_key} -> {out}\")\n",
    "\n",
    "    for k, lab in [\n",
    "        (\"SSIM_global\", \"Global SSIM (z,z+1)\"),\n",
    "        (\"PSNR\", \"PSNR (dB) (z,z+1)\"),\n",
    "        (\"MI\", \"Mutual Information (z,z+1)\"),\n",
    "        (\"NCC\", \"Normalized Cross-Correlation (z,z+1)\"),\n",
    "        (\"TV\", \"Total Variation per z-interval\"),\n",
    "    ]:\n",
    "        try:\n",
    "            plot_metric(k, lab)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Plot fail ({k}): {e}\")\n",
    "\n",
    "    print(\"\\n=== Summary (lower is better for TV metrics; higher is better for PSNR/SSIM/MI/NCC) ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    return summary_path, per_slice_csvs\n",
    "\n",
    "# ---- Execute ----\n",
    "if __name__ == \"__main__\":\n",
    "    run(paths, base_out_dir, plots_dir, spacing_z=1.0, use_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14dbfed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Output dir: /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값\n",
      "[LOAD] ProposedSynth_Epoch87_fullimage: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registformer_MrCtPelvis_ver2_ProposedSynthesis_NoRandTranslation_VoxelmorphProposedSynth_Test_Epoch87_fullimage/preds_a_1PC066.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_ProposedSynth_Epoch87_fullimage.csv\n",
      "[LOAD] MUNIT_Lcx_rotate: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/I2I_translation/MUNIT+Lcx(예전)_rotate/preds_b_1PC066.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_MUNIT_Lcx_rotate.csv\n",
      "[LOAD] VoxelMorph2D_warped: /SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registration/voxelmorph2D/warped_img_1PC066.nii.gz\n",
      "[SAVE] Per-slice metrics -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/per_slice_VoxelMorph2D_warped.csv\n",
      "[SAVE] Summary -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/inter_slice_consistency_summary.csv\n",
      "[PLOT] SSIM_global -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_SSIM_global.png\n",
      "[PLOT] PSNR -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_PSNR.png\n",
      "[PLOT] MI -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_MI.png\n",
      "[PLOT] NCC -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_NCC.png\n",
      "[PLOT] TV -> /SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값/jbhi_metrics/plot_TV.png\n",
      "\n",
      "=== Summary (lower is better for TV metrics; higher is better for PSNR/SSIM/MI/NCC) ===\n",
      "                           Name  NumSlices     TV_z    TV_xy  nTV_z_by_IQR  nTV_z_by_TVxy  Edge_TV_z  Lap_TV_z  HF_LaplacianAbs_mean  HF_LaplacianVar_mean  HF_EdgeMag_mean  PSNR_mean  SSIM_global_mean  MI_mean  NCC_mean  TV_interval_mean\n",
      "ProposedSynth_Epoch87_fullimage         56 0.018604 0.015142      0.107901       1.228630   0.073645  0.029881              0.030592              0.002929         0.159217  28.714062          0.907799 1.052093  0.902050          0.018605\n",
      "               MUNIT_Lcx_rotate         56 0.017105 0.012896      0.142341       1.326377   0.067572  0.024450              0.025258              0.002124         0.139868  28.626173          0.902232 1.108372  0.895914          0.017120\n",
      "            VoxelMorph2D_warped         56 0.014376 0.012296      0.099090       1.169133   0.056481  0.018786              0.021734              0.001692         0.139910  30.826146          0.943458 1.239177  0.940019          0.014378\n"
     ]
    }
   ],
   "source": [
    "# Unified inter-slice consistency script (local-only)\n",
    "# - TV/nTV, Edge-TV, Lap-TV, in-plane sharpness, Otsu mask, spacing_z\n",
    "# - Summary & per-slice CSVs + plots\n",
    "# - Keeps original absolute paths and filenames\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- I/O backends ----\n",
    "nib = None\n",
    "sitk = None\n",
    "try:\n",
    "    import nibabel as nib\n",
    "except Exception:\n",
    "    nib = None\n",
    "if nib is None:\n",
    "    try:\n",
    "        import SimpleITK as sitk\n",
    "    except Exception:\n",
    "        sitk = None\n",
    "\n",
    "# ---- Optional filters ----\n",
    "has_scipy = False\n",
    "try:\n",
    "    from scipy.ndimage import sobel, laplace\n",
    "    has_scipy = True\n",
    "except Exception:\n",
    "    has_scipy = False\n",
    "\n",
    "# ---- Paths (KEEP) ----\n",
    "paths = {\n",
    "    \"ProposedSynth_Epoch87_fullimage\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registformer_MrCtPelvis_ver2_ProposedSynthesis_NoRandTranslation_VoxelmorphProposedSynth_Test_Epoch87_fullimage/preds_a_1PC066.nii.gz\",\n",
    "    \"MUNIT_Lcx_rotate\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/I2I_translation/MUNIT+Lcx(예전)_rotate/preds_b_1PC066.nii.gz\",\n",
    "    \"VoxelMorph2D_warped\": r\"/SSD5_8TB/Daniel/JBHI 저널/Output(result)/MR-CT/Registration/voxelmorph2D/warped_img_1PC066.nii.gz\",\n",
    "}\n",
    "# Base output dir (KEEP)\n",
    "base_out_dir = r\"/SSD5_8TB/Daniel/JBHI 저널/Slice간의 일관성 정량값\"\n",
    "plots_dir = os.path.join(base_out_dir, \"jbhi_metrics\")\n",
    "os.makedirs(base_out_dir, exist_ok=True)\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# ---- Helpers ----\n",
    "def load_nifti(path):\n",
    "    if nib is not None:\n",
    "        img = nib.load(path)\n",
    "        return img.get_fdata(dtype=np.float32)  # (X,Y,Z)\n",
    "    elif sitk is not None:\n",
    "        img = sitk.ReadImage(path)\n",
    "        arr = sitk.GetArrayFromImage(img)  # (Z,Y,X)\n",
    "        return np.transpose(arr, (2,1,0)).astype(np.float32)\n",
    "    else:\n",
    "        raise RuntimeError(\"nibabel 또는 SimpleITK가 필요합니다.\")\n",
    "\n",
    "def robust_minmax_norm(vol, p1=1.0, p99=99.0, eps=1e-6):\n",
    "    lo = np.percentile(vol, p1)\n",
    "    hi = np.percentile(vol, p99)\n",
    "    vol = np.clip(vol, lo, hi)\n",
    "    return (vol - lo) / max(hi - lo, eps)\n",
    "\n",
    "def otsu_threshold(arr, bins=256):\n",
    "    arr = arr[np.isfinite(arr)]\n",
    "    if arr.size == 0:\n",
    "        return 0.0\n",
    "    hist, bin_edges = np.histogram(arr, bins=bins, range=(arr.min(), arr.max()))\n",
    "    w1 = np.cumsum(hist)\n",
    "    w2 = np.cumsum(hist[::-1])[::-1]\n",
    "    mu1 = np.cumsum(hist * (bin_edges[:-1] + bin_edges[1:]) / 2) / np.maximum(w1, 1e-12)\n",
    "    mu2 = (np.cumsum((hist * (bin_edges[:-1] + bin_edges[1:]) / 2)[::-1]) / np.maximum(w2[::-1], 1e-12))[::-1]\n",
    "    var12 = w1[:-1] * w2[1:] * (mu1[:-1] - mu2[1:])**2\n",
    "    idx = np.argmax(var12)\n",
    "    return float((bin_edges[idx] + bin_edges[idx+1]) / 2)\n",
    "\n",
    "# ---- Metrics ----\n",
    "def gradient_magnitude(s2d):\n",
    "    if has_scipy:\n",
    "        gx = sobel(s2d, axis=0, mode='reflect')\n",
    "        gy = sobel(s2d, axis=1, mode='reflect')\n",
    "    else:\n",
    "        gx = np.zeros_like(s2d); gy = np.zeros_like(s2d)\n",
    "        gx[1:-1,:] = (s2d[2:,:] - s2d[:-2,:]) / 2.0\n",
    "        gy[:,1:-1] = (s2d[:,2:] - s2d[:,:-2]) / 2.0\n",
    "    return np.sqrt(gx**2 + gy**2)\n",
    "\n",
    "def laplacian2d(s2d):\n",
    "    if has_scipy:\n",
    "        return laplace(s2d)\n",
    "    center = 4*s2d\n",
    "    n = np.zeros_like(s2d)\n",
    "    n[1: ,:] += s2d[:-1,:]; n[:-1,:] += s2d[1: ,:]\n",
    "    n[: ,1:] += s2d[:, :-1]; n[: ,:-1] += s2d[:, 1: ]\n",
    "    return n - center\n",
    "\n",
    "def mse(a,b): return float(np.mean((a-b)**2))\n",
    "def psnr_from_mse(m, data_range=1.0):\n",
    "    if m <= 1e-12: return float('inf')\n",
    "    return 20.0*math.log10(data_range) - 10.0*math.log10(m)\n",
    "\n",
    "def global_ssim(a,b,data_range=1.0,K1=0.01,K2=0.03):\n",
    "    a = a.astype(np.float64); b = b.astype(np.float64)\n",
    "    C1 = (K1*data_range)**2; C2 = (K2*data_range)**2\n",
    "    mu_a = float(np.mean(a)); mu_b = float(np.mean(b))\n",
    "    var_a = float(np.var(a)); var_b = float(np.var(b))\n",
    "    cov = float(np.mean((a - mu_a) * (b - mu_b)))\n",
    "    num = (2*mu_a*mu_b + C1) * (2*cov + C2)\n",
    "    den = (mu_a**2 + mu_b**2 + C1) * (var_a + var_b + C2)\n",
    "    if den == 0: return 1.0 if num == 0 else 0.0\n",
    "    return float(num/den)\n",
    "\n",
    "def mutual_information(a,b,bins=64):\n",
    "    a = np.clip(a.ravel(),0,1); b = np.clip(b.ravel(),0,1)\n",
    "    h, _, _ = np.histogram2d(a,b,bins=bins,range=[[0,1],[0,1]])\n",
    "    p = h/np.sum(h) + 1e-12\n",
    "    px = np.sum(p,axis=1,keepdims=True); py = np.sum(p,axis=0,keepdims=True)\n",
    "    return float(np.sum(p*(np.log(p) - np.log(px) - np.log(py))))\n",
    "\n",
    "def ncc(a,b):\n",
    "    a = a.astype(np.float64); b = b.astype(np.float64)\n",
    "    am, bm = np.mean(a), np.mean(b)\n",
    "    asd, bsd = np.std(a), np.std(b)\n",
    "    if asd < 1e-8 or bsd < 1e-8: return 0.0\n",
    "    return float(np.mean((a-am)*(b-bm)) / (asd*bsd))\n",
    "\n",
    "def tv_z(vol, mask=None):\n",
    "    diffs = np.abs(np.diff(vol, axis=2))\n",
    "    if mask is not None:\n",
    "        diffs = diffs * mask[:,:,1:]\n",
    "        denom = np.sum(mask[:,:,1:])\n",
    "        return float(np.sum(diffs) / max(denom, 1e-12))\n",
    "    return float(np.mean(diffs))\n",
    "\n",
    "def tv_z_per_interval(vol, mask=None):\n",
    "    diffs = np.abs(np.diff(vol, axis=2))\n",
    "    if mask is not None:\n",
    "        diffs = diffs * mask[:,:,1:]\n",
    "        denom = np.sum(mask[:,:,1:], axis=(0,1))\n",
    "        denom = np.maximum(denom, 1e-12)\n",
    "        return np.sum(diffs, axis=(0,1)) / denom\n",
    "    return np.mean(diffs, axis=(0,1))\n",
    "\n",
    "def tv_xy(vol, mask=None):\n",
    "    dx = np.abs(np.diff(vol, axis=0))\n",
    "    dy = np.abs(np.diff(vol, axis=1))\n",
    "    if mask is not None:\n",
    "        mx = mask[1:,:,:]; my = mask[:,1:,:]\n",
    "        dx = dx * mx; dy = dy * my\n",
    "        denom = np.sum(mx) + np.sum(my)\n",
    "        return float((np.sum(dx)+np.sum(dy)) / max(denom, 1e-12))\n",
    "    return float(np.mean(dx) + np.mean(dy))\n",
    "\n",
    "def edge_tv_z(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    gm = np.zeros_like(vol, dtype=np.float32)\n",
    "    for z in range(Z):\n",
    "        gm[:,:,z] = gradient_magnitude(vol[:,:,z])\n",
    "    return tv_z(gm, mask)\n",
    "\n",
    "def lap_tv_z(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    lv = np.zeros_like(vol, dtype=np.float32)gradient_magnitude\n",
    "    for z in range(Z):\n",
    "        lv[:,:,z] = laplacian2d(vol[:,:,z])\n",
    "    return tv_z(lv, mask)\n",
    "\n",
    "def inplane_sharpness_metrics(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    lap_abs, lap_var, edge_m = [], [], []\n",
    "    for z in range(Z):\n",
    "        sl = vol[:,:,z]\n",
    "        if mask is not None:\n",
    "            m = mask[:,:,z] > 0\n",
    "            if not np.any(m):\n",
    "                lap_abs.append(0.0); lap_var.append(0.0); edge_m.append(0.0); continue\n",
    "            lap = laplacian2d(sl)[m]; gm = gradient_magnitude(sl)[m]\n",
    "        else:\n",
    "            lap = laplacian2d(sl).ravel(); gm = gradient_magnitude(sl).ravel()\n",
    "        lap_abs.append(float(np.mean(np.abs(lap))))\n",
    "        lap_var.append(float(np.var(lap)))\n",
    "        edge_m.append(float(np.mean(gm)))\n",
    "    return {\n",
    "        \"LaplacianAbs_mean\": float(np.mean(lap_abs)),\n",
    "        \"LaplacianVar_mean\": float(np.mean(lap_var)),\n",
    "        \"EdgeMag_mean\": float(np.mean(edge_m)),\n",
    "    }\n",
    "\n",
    "def per_slice_pair_metrics(vol, mask=None):\n",
    "    Z = vol.shape[2]\n",
    "    d = {\"MSE\":[], \"PSNR\":[], \"SSIM_global\":[], \"MI\":[], \"NCC\":[], \"TV\":[]}\n",
    "    intervals = tv_z_per_interval(vol, mask)\n",
    "    for z in range(Z-1):\n",
    "        a = vol[:,:,z]; b = vol[:,:,z+1]\n",
    "        if mask is not None:\n",
    "            m = (mask[:,:,z] * mask[:,:,z+1]) > 0\n",
    "            a_sel = a[m] if np.any(m) else a\n",
    "            b_sel = b[m] if np.any(m) else b\n",
    "        else:\n",
    "            a_sel = a; b_sel = b\n",
    "        mv = mse(a_sel, b_sel)\n",
    "        d[\"MSE\"].append(mv)\n",
    "        d[\"PSNR\"].append(psnr_from_mse(mv))\n",
    "        d[\"SSIM_global\"].append(global_ssim(a_sel, b_sel))\n",
    "        d[\"MI\"].append(mutual_information(a_sel, b_sel))\n",
    "        d[\"NCC\"].append(ncc(a_sel, b_sel))\n",
    "        d[\"TV\"].append(float(intervals[z]))\n",
    "    for k in d.keys():\n",
    "        d[k] = np.array(d[k], dtype=np.float64)\n",
    "    return d\n",
    "\n",
    "def summarize_all(vol, name, spacing_z=1.0, use_mask=True):\n",
    "    vol_n = robust_minmax_norm(vol)\n",
    "    mask = None\n",
    "    if use_mask:\n",
    "        thr = otsu_threshold(vol_n)\n",
    "        mask = (vol_n > thr).astype(np.float32)\n",
    "\n",
    "    tvz = tv_z(vol_n, mask)\n",
    "    tvxy = tv_xy(vol_n, mask)\n",
    "\n",
    "    nz = vol_n[vol_n > 0]\n",
    "    iqr = float(np.percentile(nz,75) - np.percentile(nz,25)) if nz.size > 0 else 1.0\n",
    "    nTV_iqr = tvz / max(iqr, 1e-6)\n",
    "    nTV_xy  = tvz / max(tvxy, 1e-6)\n",
    "\n",
    "    edge_tv = edge_tv_z(vol_n, mask)\n",
    "    lap_tv  = lap_tv_z(vol_n, mask)\n",
    "\n",
    "    curves = per_slice_pair_metrics(vol_n, mask)\n",
    "    sharp  = inplane_sharpness_metrics(vol_n, mask)\n",
    "\n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"NumSlices\": int(vol.shape[2]),\n",
    "        \"TV_z\": tvz / max(spacing_z, 1e-6),  # spacing 보정\n",
    "        \"TV_xy\": tvxy,\n",
    "        \"nTV_z_by_IQR\": nTV_iqr,\n",
    "        \"nTV_z_by_TVxy\": nTV_xy,\n",
    "        \"Edge_TV_z\": edge_tv,\n",
    "        \"Lap_TV_z\": lap_tv,\n",
    "        \"HF_LaplacianAbs_mean\": sharp[\"LaplacianAbs_mean\"],\n",
    "        \"HF_LaplacianVar_mean\": sharp[\"LaplacianVar_mean\"],\n",
    "        \"HF_EdgeMag_mean\": sharp[\"EdgeMag_mean\"],\n",
    "        \"PerSlice_Metrics\": curves,\n",
    "    }\n",
    "\n",
    "# ---- Runner ----\n",
    "def run(paths, base_out_dir, plots_dir, spacing_z=1.0, use_mask=True):\n",
    "    os.makedirs(base_out_dir, exist_ok=True)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "    summaries = []\n",
    "    per_slice_csvs = {}\n",
    "\n",
    "    print(f\"[INFO] Output dir: {base_out_dir}\")\n",
    "    for name, p in paths.items():\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Missing file: {p}\")\n",
    "            continue\n",
    "        print(f\"[LOAD] {name}: {p}\")\n",
    "        vol = load_nifti(p)\n",
    "        if vol.ndim != 3:\n",
    "            raise ValueError(f\"{name}: expected 3D, got {vol.shape}\")\n",
    "\n",
    "        S = summarize_all(vol, name, spacing_z=spacing_z, use_mask=use_mask)\n",
    "        summaries.append(S)\n",
    "\n",
    "        # per-slice CSV (KEEP location/pattern)\n",
    "        df = pd.DataFrame({k: v for k, v in S[\"PerSlice_Metrics\"].items()})\n",
    "        per_csv = os.path.join(base_out_dir, f\"per_slice_{name}.csv\")\n",
    "        df.to_csv(per_csv, index=False)\n",
    "        per_slice_csvs[name] = per_csv\n",
    "        print(f\"[SAVE] Per-slice metrics -> {per_csv}\")\n",
    "\n",
    "    if len(summaries) == 0:\n",
    "        raise RuntimeError(\"No volumes processed. Check file paths.\")\n",
    "\n",
    "    # Summary CSV (KEEP filename)\n",
    "    rows = []\n",
    "    for S in summaries:\n",
    "        rows.append({\n",
    "            \"Name\": S[\"Name\"],\n",
    "            \"NumSlices\": S[\"NumSlices\"],\n",
    "            \"TV_z\": S[\"TV_z\"],\n",
    "            \"TV_xy\": S[\"TV_xy\"],\n",
    "            \"nTV_z_by_IQR\": S[\"nTV_z_by_IQR\"],\n",
    "            \"nTV_z_by_TVxy\": S[\"nTV_z_by_TVxy\"],\n",
    "            \"Edge_TV_z\": S[\"Edge_TV_z\"],\n",
    "            \"Lap_TV_z\": S[\"Lap_TV_z\"],\n",
    "            \"HF_LaplacianAbs_mean\": S[\"HF_LaplacianAbs_mean\"],\n",
    "            \"HF_LaplacianVar_mean\": S[\"HF_LaplacianVar_mean\"],\n",
    "            \"HF_EdgeMag_mean\": S[\"HF_EdgeMag_mean\"],\n",
    "            \"PSNR_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"PSNR\"])),\n",
    "            \"SSIM_global_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"SSIM_global\"])),\n",
    "            \"MI_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"MI\"])),\n",
    "            \"NCC_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"NCC\"])),\n",
    "            \"TV_interval_mean\": float(np.mean(S[\"PerSlice_Metrics\"][\"TV\"])),\n",
    "        })\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    summary_path = os.path.join(base_out_dir, \"inter_slice_consistency_summary.csv\")\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"[SAVE] Summary -> {summary_path}\")\n",
    "\n",
    "    # Plots (save into jbhi_metrics subfolder)\n",
    "    def plot_metric(metric_key, ylabel):\n",
    "        plt.figure()\n",
    "        for S in summaries:\n",
    "            y = S[\"PerSlice_Metrics\"][metric_key]\n",
    "            x = np.arange(1, len(y)+1)\n",
    "            plt.plot(x, y, label=S[\"Name\"])\n",
    "        plt.xlabel(\"Adjacent slice index (z → z+1)\")\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        out = os.path.join(plots_dir, f\"plot_{metric_key}.png\")\n",
    "        plt.savefig(out, dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"[PLOT] {metric_key} -> {out}\")\n",
    "\n",
    "    for k, lab in [\n",
    "        (\"SSIM_global\", \"Global SSIM (z,z+1)\"),\n",
    "        (\"PSNR\", \"PSNR (dB) (z,z+1)\"),\n",
    "        (\"MI\", \"Mutual Information (z,z+1)\"),\n",
    "        (\"NCC\", \"Normalized Cross-Correlation (z,z+1)\"),\n",
    "        (\"TV\", \"Total Variation per z-interval\"),\n",
    "    ]:\n",
    "        try:\n",
    "            plot_metric(k, lab)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Plot fail ({k}): {e}\")\n",
    "\n",
    "    print(\"\\n=== Summary (lower is better for TV metrics; higher is better for PSNR/SSIM/MI/NCC) ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    return summary_path, per_slice_csvs\n",
    "\n",
    "# ---- Execute ----\n",
    "if __name__ == \"__main__\":\n",
    "    run(paths, base_out_dir, plots_dir, spacing_z=1.0, use_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f39efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11594d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "danny_natten",
   "language": "python",
   "name": "danny_natten"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
